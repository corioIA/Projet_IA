{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/corioIA/Projet_IA/blob/main/IA_bat.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PROJET D'IA\n",
        "\n",
        "I- Introduction\n",
        "\n",
        "Dans la continuité du cours du premier semestre \"Introduction à l'IA et à la data science\" nous avons du réalisé un projet de maching learning supervisé dans le but de concevoir, développer et presenter un modèle d\"apprentissage automatique. Notre projet consite à prévoir la durée de vie résiduelle des batteries lithium-ion. Nous avions pour cela un esemble de données non-traitées à notre disposition. Vous trouverez dans ce rapport notre démarche expliquée ainsi que notre reflexion. En plus du texte, les lignes de code seront commentées et font parties du rapport.  \n",
        "\n",
        "II- Prise en main du Data set\n",
        "\n",
        "Dans un premier temps, nous devions nous familiariser avec le Dataset. Pour cela nous devions d'abord trouver où notre document python était pour mettre au même endroit notre dataset."
      ],
      "metadata": {
        "id": "eIQ-JS1_fYYO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "collapsed": true,
        "id": "lAiyUorPRoZu",
        "outputId": "8ee719b3-3464-4e89-946b-77e0fdf3b2dc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "pwd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "La commande dans la cellule ci-dessus permet de savoir où est situé notre document python dans colab et ainsi, nous permet de télécharger le data set au même endroit que notre code pour pouvoir l'ouvrir.\n",
        "La premiere cellule d'un projet contient toujours les bibliothèques que nous avons utilisées pour le projet. Comme vous pouvez le voir ci-dessous les principales sont :\n",
        "- Numpy permettant d'effectuer des calculs logiques et mathématiques sur des tableaux et des matrices,\n",
        "- Pandas permettant une structure de données efficace et rapide pour la manipulation des données avec indexation intégrée,\n",
        "- Matplotlib et Seaborn permettant la creation de differents graphiques\n",
        "- Sklearn permettant l'importation du PCA et de différents modèles pour notre projet.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gFHaFGvukXS4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import sklearn as skl\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n"
      ],
      "metadata": {
        "id": "4IEMP0_eR6KE"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv('Battery_RUL.csv')#Permet la lecture du fichier csv à l'aide de la fonction read de pandas et le tansforme directement en datframe\n",
        "\n",
        "df.head(10)#Permet d'afficher les 10 permieres ligne du dataframe"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "collapsed": true,
        "id": "pawwR0oMTic3",
        "outputId": "be389b45-09a3-44ff-b4b2-25350a331723"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'Battery_RUL.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-664cf9f1187a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Battery_RUL.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#Permet la lecture du fichier csv à l'aide de la fonction read de pandas et le tansforme directement en datframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#Permet d'afficher les 10 permieres ligne du dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Battery_RUL.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape# Nous permet d'avoir le nombre de colonne et lignes du dataframe\n"
      ],
      "metadata": {
        "id": "OITLaCbQVDkr",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()#Nous donne diffèrentes information sur le dataframe telle que le rangeIndex le Datatype ou la mémoire utilisée."
      ],
      "metadata": {
        "collapsed": true,
        "id": "QolycGPTvQ4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()#Permet de décrire le dataframe et perment de nous projeter sur le nettoyage de celui-ci"
      ],
      "metadata": {
        "id": "8bJyWd6XvQ8C",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Après avoir pris en main le dataframe à l'aide de différentes méthodes des différentes bibliothèques, nous avons maintenant une meilleure vue d'ensemble sur nos données. Nous devons maintenant passer à l'étape la plus cruciale : le nettoyage de la base de données.\n",
        "\n",
        "\n",
        "III- Le Nettoyage et analyse\n",
        "\n",
        "\n",
        "Le nettoyage d'un data set permet de supprimer les valeurs pouvant dégrader la performance de nos modèles choisis.\n",
        "Les différents éléments à nettoyer sont les doublons, les valeurs aberrantes et les valeurs manquantes.\n",
        "\n",
        "Nous avons choisi de commencer par la suppression des valeurs aberrantes. Pour cela, nous avons effectué une analyse descriptive sur chacune de nos variables à l'aide de la methode boxplot de la bibliothèque seaborn.\n"
      ],
      "metadata": {
        "id": "Y8ExglZspX7G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Analyse descriptive de la variable Cycle_Index\n",
        "sns.boxplot(data=df, y=\"Cycle_Index\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "2SK5hQAlYCU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La variable \"Cycle index\" ne requiert aucun nettoyage, nous n'avons pas detecté de valeurs aberrantes. La deuxième variable \"Discharge time (s)\" nécessite un nettoyage. En effet, nous remarquons que celle-ci contient des valeurs négatives. Or, un temps ne peut être négatif. Après cela nous nous sommes renseignés sur le temps de décharge moyen d'une batterie lithium-ion. Nous avons remarqué que comparer à cette moyenne les valeurs max et min étaient beaucoup trop divergentes. Nous avons donc décidé de ne garder que les valeur au dessus de 1150 secondes et en dessous de 2500 secondes. Effectivement, l'utilisateur n'utilisera plus sa batterie à partir de certainnes performaces. Il n'attendra pas par exemple que celle-ci dure seulement 5 secondes pour la changer, d'où notre encadrement assez élevé."
      ],
      "metadata": {
        "id": "T_Nz6F5VsiJv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df=df[df['Discharge Time (s)']>=1150]\n",
        "df=df[df['Discharge Time (s)']<=2500]\n",
        "df.describe()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "IJulPdvRcssE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Analyse descriptive de la variable Discharge Time\n",
        "sns.boxplot(data=df, y=\"Discharge Time (s)\")"
      ],
      "metadata": {
        "id": "JywDwI1txvxR",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "De même pour la variable \"Decrement 3.6-3.4V (s)\" nous remarquons des valeurs de temps négatives que nous supprimons également."
      ],
      "metadata": {
        "id": "I_3SJGuVyBtG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Analyse descriptive de la variable Decrement 3.6-3.4V\n",
        "sns.boxplot(data=df, y=\"Decrement 3.6-3.4V (s)\")"
      ],
      "metadata": {
        "id": "1O7-2GDvyGQT",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Suppression des valuers abérantes.\n",
        "df=df[df['Decrement 3.6-3.4V (s)']>=0] # on affecte au dataframe toute les valeur qui n'ont pas de Decrement 3.6-3.4V (s) < 0 ce qui supprime ainsi toute les valeurs negative. Un temps négatif n'existant pas\n",
        "#df.describe()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "AHtYKw31lR5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Analyse descriptive de la variable Max. Voltage Dischar.\n",
        "sns.boxplot(data=df, y=\"Max. Voltage Dischar. (V)\")"
      ],
      "metadata": {
        "id": "GdduRcaGyWru",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La variable \"Max. Voltage Dischar. (V)\" et La variable \"Min. Voltage Charg. (V)\" ne semble pas avoir de valeurs aberrantes. Nous avons donc décidé de ne rien supprimer pour ces deux variables."
      ],
      "metadata": {
        "id": "GWqbG8xHyjpf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Analyse descriptive de la variable Min. Voltage Charg.\n",
        "sns.boxplot(data=df, y=\"Min. Voltage Charg. (V)\")"
      ],
      "metadata": {
        "id": "4YV_oVRoye3k",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Analyse descriptive de la variable Time at 4.15V\n",
        "sns.boxplot(data=df, y=\"Time at 4.15V (s)\")"
      ],
      "metadata": {
        "id": "MkOzboqayv_U",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La variable 'Time at 4.15V (s)' contient  aussi des valeurs de temps négatives que nous supprimons."
      ],
      "metadata": {
        "id": "gIOCoSSfzRu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Suppression des valeurs abérantes de la colonne Time at 4.15V (s).\n",
        "df=df[df['Time at 4.15V (s)']>=0]\n",
        "sns.boxplot(data=df, y=\"Time at 4.15V (s)\")# On replot la box pour voir les effets de nos modifications\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ShNqmytEnrc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous avons décidé de ne pas toucher à la variable \"Time constant current (s)\" qui semble correcte."
      ],
      "metadata": {
        "id": "E2ZMZzjoga7r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Analyse descriptive de la variable Time constant current\n",
        "sns.boxplot(data=df, y=\"Time constant current (s)\")"
      ],
      "metadata": {
        "id": "taxC0RDNy2Uh",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La variable \"Charging time (s)\" contenait elle aussi des valeurs de temps négatives. Et comme pour la variable \"Discharge Time (s)\", nous avons decidé de créer un encadrement plus précis pour pouvoir garder que des valeurs améliorant notre data set et ainsi pouvoir mieux prédire la duréé de vie résiduelle d'une batterie. Nous nous sommes donc renseignés sur le temps de charge moyen d'une batterie lithium-ion et nous avons trouvé que les valeurs maximum et minimum étaient soit beaucoup trop hautes soit beaucoup trop faibles. Par exemple la valeur minimal après avoir supprimé les valeurs négatives était de 5 secondes ce qui en toute logique est beaucoup trop faible pour recharger une batterie. Après reflexion, nous sommes donc arrivé à l'encadrement dans la cellule ci-dessous qui est entre 7200 seconde et 9750 seconde."
      ],
      "metadata": {
        "id": "fQr_zKr4gp1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Suppression des valeurs abérantes.\n",
        "df=df[df['Charging time (s)']>=7200]\n",
        "df=df[df['Charging time (s)']<=9750]\n",
        "\n",
        "#df.describe()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "dUlfLcprdMVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Analyse descriptive de la variable Charging time (s).\n",
        "sns.boxplot(data=df, y=\"Charging time (s)\")"
      ],
      "metadata": {
        "id": "9zAsuPgOzAea",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La variable \"RUL\" n'a pas eu besoin d'être nettoyé."
      ],
      "metadata": {
        "id": "QGyX3FKMjYib"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Analyse descriptive de la variable RUL\n",
        "sns.boxplot(data=df, y=\"RUL\")"
      ],
      "metadata": {
        "id": "U2aYE9khzIdW",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dans la cellule ci-dessous la méthode duplicate nous permet de trouver les doublons présents dans le data set. Et, comme indiqué dans la cellule, notre data set ne contient aucun doublon."
      ],
      "metadata": {
        "id": "AttxhJEYjiyS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Recherche de doublon dans le dataSet\n",
        "df.duplicated().sum() # Il n'y a pas de doublon"
      ],
      "metadata": {
        "id": "2_k5vUAKzbru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous devions ensuite créer une matrice de corrélation. Celle- ci permet comme son nom l'indique de voir la corrélation entre  les diffèrentes variables de notre data set. Si les valeurs sont entre 0,7 et 1 valeur alors il y a une forte corrélation entre les deux variables et entre 0 et 0.7 ont avec une mauvaise corrélation entre les valeurs. Idem pour les valeurs négatives. Pour créer la matrice nous avons utilisé la bibliothèque seaborn qui nous explique à l'aide de sa documentaton comment la réaliser. Les commentaires dans la cellule ci-dessous explique les différentes étapes de création."
      ],
      "metadata": {
        "id": "PmdSV1mGkAvP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Matrice de corrélation\n",
        "corr = df.corr()\n",
        "\n",
        "# Génere un masque pour le triangle supérieur\n",
        "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
        "\n",
        "# Crée un graphe matplotlib\n",
        "f, ax = plt.subplots(figsize=(11, 9))\n",
        "\n",
        "# Geénère une carte de couleur divergente\n",
        "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
        "\n",
        "# Dessine la heatmap avec un mask et un aspect ration corrigé\n",
        "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0, annot=True,\n",
        "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
      ],
      "metadata": {
        "id": "J5bWoXX-1Zid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous avons ensuite tracé des courbes bi-variable. Pour pouvoir tirer des résultats, nous avons tracé des valeurs fortement corrélées pour pouvoir tirer des info des courbes et pouvant nous donné des intuitions pour le feature engineering. Le RUL étant notre target et la Variable \"Cycle Index\" n'étant pas utile dans nos modèles nous ne traçons pas de courbes avec ces deux variables. Plus blabla sur les courbes et rajouter des courbes"
      ],
      "metadata": {
        "id": "8T_2co79p3Ed"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Courbe bivariable\n",
        "\n",
        "sns.scatterplot(data=df,x='Discharge Time (s)',y='Time at 4.15V (s)')\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "FLuDCn7hfB3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.scatterplot(data=df,x='Discharge Time (s)',y='Decrement 3.6-3.4V (s)')"
      ],
      "metadata": {
        "id": "tbfX7O3jvoL_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.scatterplot(data=df,x='Time at 4.15V (s)',y='Time constant current (s)')"
      ],
      "metadata": {
        "id": "5pUTC6otwAwS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.scatterplot(data=df,x='Discharge Time (s)',y='Charging time (s)')"
      ],
      "metadata": {
        "id": "t8i17dd1JHEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.scatterplot(data=df,x='Discharge Time (s)',y='Time constant current (s)')"
      ],
      "metadata": {
        "id": "l0mx4ulHI4YY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "III- Feature Engineering\n",
        "\n",
        "Le feature engineering consiste à créer de nouvelles variables nous permettant de rendre plus efficace et plus précis notre ensemble d'étude et ainsi obtenir un modèle d'une meilleure qualité. Pour cela il faut trouver des liens entre les différentes variables descriptives de notre système pour effectuer, entre elles des opérations mathématiques pour en obtenir de nouvelles.\n",
        "Dans certains cas, il est possible de ne pas faire de feature engineering. Ici, après l'étude de la matrice de corrélation et des graphes bi-variés, nous observons une relation de proportionnalité (courbe linéaire) entre Discharge time et Time at 4.15V ainsi que Discharge time et Time constant current. Sachant que la variable Discharge Time est un indicateur de la santé de notre batterie (plus il diminue plus la batterie présente des signes de faiblesse) et, étant fortement corrélé avec les deux autres variables citées précédemment, il nous a paru juste de créer nos deux features présentent dans la cellule ci-dessous.\n",
        "Le ratio créé indique la tendance suivie par nos variables, cela ajoute donc une information complémentaire importante quant à la compréhension de notre dataset."
      ],
      "metadata": {
        "id": "0n_w7kLbqdyU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Feature engineering\n",
        "#Ratio discharge/time at 4.15 V\n",
        "#Ratio discharge/time at constant courant\n",
        "df['Ratio_Discharge_4.15V']=df['Discharge Time (s)']/df['Time at 4.15V (s)']\n",
        "df['Ratio_Discharge_Constant_Current']=df['Discharge Time (s)']/df['Time constant current (s)']\n",
        "df.drop('Cycle_Index',axis=1,inplace=True)# La variable Cycle index ne sert pas pour la modélisation.\n",
        "df.head()\n",
        "\n"
      ],
      "metadata": {
        "id": "bFNvyXvXgonw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rul=df['RUL']\n",
        "df.drop('RUL',axis=1,inplace=True)\n",
        "rul.shape"
      ],
      "metadata": {
        "id": "zQFZ2VzxWcm9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dans la cellule ci-dessus, nous séparons notre target de notre ensemble de données. Il est nécessaire de faire cela au préalable pour pas que notre target entre dans les variables que notre modèle devra traiter pour prédire un résultat.\n",
        "\n",
        "Dans la cellule ci-dessous, nous créons notre dataset d'entrainement et notre dataset test ainsi que ceux de la target. L'ensemble d'entrainement est composé à 80% de nos données. Les autres 20% étant pour le test."
      ],
      "metadata": {
        "id": "kI1tterGBQ2N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# création du data set d'entrainement et du data set de test\n",
        "\n",
        "df_train, df_test, rul_train, rul_test = train_test_split(df,rul,train_size=0.8 ,test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "3lNhm75RTqM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pour la suite de notre feature engineering, il faut normaliser les variables du dataset.\n",
        "Nous n'avons pas normalisé la target car cela n'entrait pas dans notre champ de compétences. Mais il est d'usage de normaliser celle-ci."
      ],
      "metadata": {
        "id": "zcsBqGp6CLFA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#normalisation\n",
        "scaler = skl.preprocessing.StandardScaler() # Create a StandardScaler instance\n",
        "scaled_data_train = scaler.fit_transform(df_train) # Fit and transform the data\n",
        "scaled_data_test = scaler.fit_transform(df_test)"
      ],
      "metadata": {
        "id": "PddjA5uSEZRc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dans la case ci-dessous, nous allons procéder à l'ACP. L'objectif principal de l'ACP est de transformer un jeu de données avec de nombreuses variables en un jeu de données avec moins de variables tout en préservant autant que possible l'information présente dans les données originales. Cela permet de rendre l'analyse plus rapide et plus facile, sans perdre trop d'informations pertinentes. Ceci correspond par exemple à zipper un dossier.\n",
        "L'ACP se fait à l'aide de la bibliothèque sklearn importée précedemment.\n",
        "L'un des problème rencontré a été de créer notre ACP avec la méthode .predict qui n'existe pas dans cette bibliothèque. En réaction, nous avons utilisé la méthode .transform. Pour le dataset d'entrainement, il faut utiliser la méthode .fit_transform."
      ],
      "metadata": {
        "id": "8yAi9WH-DPVN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#df_test_ACP=ACP.predict(df_scaled_test)\n",
        "\n",
        "def perform_pca_analysis(X, n_components=0.95):\n",
        "    pca = PCA(n_components=n_components)\n",
        "    X_pca = pca.fit_transform(X)\n",
        "\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    # Variance expliquée cumulée\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(np.cumsum(pca.explained_variance_ratio_), 'bo-')\n",
        "    plt.xlabel('Nombre de composantes')\n",
        "    plt.ylabel('Variance expliquée cumulée')\n",
        "    plt.title('Variance expliquée cumulée par composante')\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Scree plot\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(pca.explained_variance_ratio_, 'ro-')\n",
        "    plt.xlabel('Composantes')\n",
        "    plt.ylabel('Variance expliquée individuelle')\n",
        "    plt.title('Scree Plot')\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return pca, X_pca\n",
        "\n",
        "\n",
        "pca, train_pca = perform_pca_analysis(scaled_data_train )\n",
        "\n",
        "\n",
        "train_pca.shape\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YUFpKb8fPHht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_pca= pca.transform(scaled_data_test)#ensemble de données du test aprés tranformation ACP\n",
        "test_pca.shape"
      ],
      "metadata": {
        "id": "emwTAk8ZqV4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "IV- Moélisation et évaluation\n",
        "\n",
        "\n",
        "Lors de notre modélisation, nous avons réalisé 3 modèles :\n",
        " - Une régression linéaire\n",
        " - Une random forest\n",
        " - Une gradient boost\n",
        "\n",
        " Pour évaluer les performances de nos modèles, nous avons choisi d'utiliser les métriques d'évaluation :      \n",
        "  - MAE (mesure l'écart moyen entre les valeurs réelles et les valeurs prédites, sans tenir compte du signe de l'erreur)\n",
        "  - MSE (mesure la différence entre les valeurs réelles et les valeurs prédites par le modèle)\n",
        "  - RMSE (est la racine carrée de la MSE)\n",
        "  - r2 (mesure la fiabilité de notre modèle)\n",
        "\n",
        "  Nous ferons maintenant une analyse de chaque modèles et de leurs metriques."
      ],
      "metadata": {
        "id": "t70q32wMrUHU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#modeles\n",
        "model=LinearRegression()\n",
        "model.fit(train_pca, rul_train)\n",
        "rul_pred=model.predict(test_pca)\n",
        "mae=mean_absolute_error(rul_test,rul_pred)\n",
        "mse=mean_squared_error(rul_test,rul_pred)\n",
        "rmse=np.sqrt(mse)\n",
        "r2=r2_score(rul_test,rul_pred)\n",
        "print(mae)\n",
        "print(mse)\n",
        "print(rmse)\n",
        "print(r2)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CcNCjWeHEdrm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pour prendre en main la modélisations de modèle, nous avons choisi de commencé par une regression linéaire.\n",
        "\n",
        "Ici, notre MAE est de 36.24. Ce qui signifie que, en moyenne, notre modèle fait une erreur de 36.24 cycles dans ses prédictions de durée de vie de la batterie. Cela donne une idée directe de l'écart moyen entre les prédictions et les valeurs réelles.\n",
        "\n",
        "De plus, une MSE de 2342.10 suggère que notre modèle a des erreurs importantes (en termes de carré des écarts) qui, par rapport à d'autres modèles ou approches, pourraient être considérées comme assez grandes. Cependant, bien que la MSE soit plus grande que la MAE, ce n'est pas nécessairement un problème, car elle est plus sensible aux grandes erreurs. Cela peut indiquer que notre modèle a quelques prédictions qui sont bien en dehors de la réalité.\n",
        "\n",
        "En outre, le RMSE est simplement la racine carrée de la MSE, ce qui le ramène à l'échelle des données d'origine. Cela donne une idée de l'erreur moyenne en termes d'unités de la variable cible. Un RMSE de 48.40, indique que l'écart-type des erreurs est d'environ 48.40 cycles.\n",
        "\n",
        "Pour finir, un r2 de 0,96 nous indique que notre modèle est fiable à 96%. Il est donc très performant.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ceHwg9o_KDQX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "model.fit(train_pca, rul_train)\n",
        "rul_pred = model.predict(test_pca)\n",
        "mae=mean_absolute_error(rul_test,rul_pred)\n",
        "mse=mean_squared_error(rul_test,rul_pred)\n",
        "rmse=np.sqrt(mse)\n",
        "r2=r2_score(rul_test,rul_pred)\n",
        "print(mae)\n",
        "print(mse)\n",
        "print(rmse)\n",
        "print(r2)\n"
      ],
      "metadata": {
        "id": "-kPHetpQ2iY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " La random forest consiste en un ensemble d'arbres de décision qui travaillent ensemble pour fournir des prédictions plus robustes et plus précises que les arbres de décision individuels.\n",
        " La Random Forest est une méthode d'ensemble, ce qui signifie qu'elle combine plusieurs modèles pour améliorer la performance globale. Dans le cas du Random Forest, ce sont des arbres de décision qui sont combinés pour former un \"forêt\" de décisions.\n",
        "Le modèle utilise des hyperparamètres. Nous expliqueront dans la partie V ce qu'est un hyperparamètre et comment nous les avons optimiser.\n",
        "Dans un prmeir temps les hyperamètres selectionnés sont ceux par défaut.\n",
        "\n",
        "Nous obtenons les métriques suivant:\n",
        "- Une MAE de 20,7, ce qui signifie que notre système se trompe de 20,7 cycles de durée de vie en moyenne\n",
        "- Une MSE de 1264 nous indique que notre modèle possède des erreurs assez élevées en moyenne et qu'il est sensible aux grandes erreurs.\n",
        "- Une RMSE de 35,5 cycles signifie que nous avons un écart-type des erreurs qui est en moyenne de 35,5 cycles.\n",
        "- Enfin, une r2 de 0,98 nous montre quze notre modèle est fiable à 98% et qu'il possède donc une grande performance.\n",
        "\n",
        "Comparé au modèle de régression linéaire ce modèle est plus performant.\n"
      ],
      "metadata": {
        "id": "duBBIWKcPFYb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
        "model.fit(train_pca, rul_train)\n",
        "rul_pred = model.predict(test_pca)\n",
        "mae=mean_absolute_error(rul_test,rul_pred)\n",
        "mse=mean_squared_error(rul_test,rul_pred)\n",
        "rmse=np.sqrt(mse)\n",
        "r2=r2_score(rul_test,rul_pred)\n",
        "# Affichage des résultats\n",
        "print(f\"MAE: {mae}\")\n",
        "print(f\"MSE: {mse}\")\n",
        "print(f\"RMSE: {rmse}\")\n",
        "print(f\"R²: {r2}\")"
      ],
      "metadata": {
        "id": "2_v6Z959rlsQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pour le 3ème modèle, nous avons choisi de faire un gradient boosting regressor. Cet estimateur construit un modèle additif de manière progressive, étape par étape ; il permet d'optimiser des fonctions de perte différentiables arbitraires. À chaque étape, un arbre de régression est ajusté sur le gradient négatif de la fonction de perte donnée.\n",
        "Tout come le modèle précédent, celui-ci utilise des hyperparamètres défini par défaut.\n",
        "Nous obtenons les métriques suivant:\n",
        "- Une MAE de 29,8, ce qui signifie que notre système se trompe de 29,8 cycles de durée de vie en moyenne\n",
        "- Une MSE de 1705,6 nous indique que notre modèle possède des erreurs assez élevées en moyenne et qu'il est sensible aux grandes erreurs.\n",
        "- Une RMSE de 41,3 cycles signifie que nous avons un écart-type des erreurs qui est en moyenne de 41,3 cycles.\n",
        "- Enfin, une r2 de 0,97 nous montre quze notre modèle est fiable à 98% et qu'il possède donc une grande performance.\n",
        "\n",
        "Nous obtenons des résultats similaires à notre model random forest. Bien que ceux-ci soient moins bons.\n",
        "\n",
        "\n",
        "Dans leur globalité, nos trois modèles obtiennent des résultats satisfaisant bien que nous ayons une MSE assez élevée. Ce métrique peut être amélioré grâce à l'optimisation des hyperparamètres.\n",
        "Cela traduit notre bonne compréhension du sujet et notre bon traitement du dataset."
      ],
      "metadata": {
        "id": "NX6x4KVZSFkX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "V-Hyperparamètre\n",
        "\n",
        "\n",
        "Les hyperparamètres sont des paramètres que l'utilisateur doit spécifier et qui influencent la façon dont le modèle va apprendre à partir des données.\n",
        "Nous choississons de faire cette optimisation sur le modèle random forest (Celui-ci étant notre modèle le plus performant).\n",
        "\n",
        "De plus, lors de l'optimisation des hyperparamètres, il est crucial d'utiliser la validation croisée pour éviter le sur-apprentissage. Cela permet de s’assurer que le modèle est bien évalué sur différentes portions de données et qu’il peut généraliser de manière fiable sur des données non vues.\n",
        "\n",
        "Dans scikit-learn, l'option cv dans GridSearchCV et RandomizedSearchCV effectue automatiquement la validation croisée. Nous en faisons 3.\n",
        "\n",
        "Nous choisissons d'optimiser les paramètres suivant:\n",
        "\n",
        "- n_estimators, le nombre d'arbre\n",
        "- max_depth, la profondeur de la fôret\n",
        "- min_samples_split, le nombre minimum d'echantillon permettant de spérare un noeud interne\n",
        "- min_samples_leaf, le minimum d'échatillon pour former un feuille\n",
        "- max_feature, le nombre de feature à considérer pour avoir la meilleure séparation.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "g1G0L9fWramJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model=RandomForestRegressor()\n",
        "\n",
        "param_dist={'n_estimators': np.arange(100, 500, 150), \"max_depth\" : [5,10,15,None], \"min_samples_split\" : [2, 5, 10], \"min_samples_leaf\" : [1, 2, 4], 'max_features': ['auto', 'sqrt', 'log2']}\n",
        "\n",
        "model_opti=RandomizedSearchCV(estimator=model, param_distributions=param_dist, n_iter=75, cv=3, n_jobs=-1)\n",
        "model_opti.fit(train_pca, rul_train)\n",
        "print(model_opti.best_params_)"
      ],
      "metadata": {
        "id": "E1k1YkeWXiV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = RandomForestRegressor(n_estimators=400, min_samples_split=2,min_samples_leaf=1, max_features='sqrt' ,random_state=42)\n",
        "model.fit(train_pca, rul_train)\n",
        "rul_pred = model.predict(test_pca)\n",
        "mae=mean_absolute_error(rul_test,rul_pred)\n",
        "mse=mean_squared_error(rul_test,rul_pred)\n",
        "rmse=np.sqrt(mse)\n",
        "r2=r2_score(rul_test,rul_pred)\n",
        "print(mae)\n",
        "print(mse)\n",
        "print(rmse)\n",
        "print(r2)"
      ],
      "metadata": {
        "id": "cdK7PoVWfpTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "L'optimisation des hyperparmètres étant trés couteuse en temps nous avons optimiser que certains paramètres et fait un nombre faible d'itération. L'optimisation nous donnant des valeurs de métriques trés proche de celle obtenues sans optimisation, nous concluons que notre optimisation n'est pas satisfaisante et qu'il faudrait soit plus de temps pour tester plus de combinaisons soit une plus grande puissance de calcul sachant que nous utilisons déja tout les coeurs disponiblent. Cependant, nous arrivons quand même à diminuer la MSE."
      ],
      "metadata": {
        "id": "CzjwpcQ1ggv3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "VI- Analyse des résultats\n",
        "\n",
        "Pour ce projet d'IA, nous devions créer un programme à partir d'un dataset pour estimer la durée de vie résiduel d'une batterie lithium-ion.\n",
        "Nous avons procédé en suivant les étapes d'un projet d'IA à savoir :\n",
        "- Nettoyage du dataset\n",
        "- feature engineering\n",
        "- Préparation du dataset (test et entrainement)\n",
        "- Modélisation\n",
        "- Optimisation\n",
        "- Evaluation\n",
        "Au cours de l'étape de compréhension des données, nous avons trouvé que les variables les plus importantes pour avoir une bonne estimation étaient :    \n",
        "- Discharge Time\n",
        "- Time at 4.15V\n",
        "- Time at constant current\n",
        "\n",
        "Ensuite, nous avons fait le choix de trois modèles différents :    \n",
        "- Regression\n",
        "- Random forest regression\n",
        "- Grandient boost\n",
        "\n",
        "En évaluant ces trois modèles nous avons déterminer que le meilleur était le random forest regression. Nous avons ensuite procédé à une optimisation de ses hyperparamètres.\n",
        "Nous obtenons une évaluation très pertinentes avec par exemple un r2 de 0,98 ce qui reflète bien la fiabilité de notre modèle. De plus notre MAE est de 22,4 ce qui signifie que notre modèle se trompe de 22,4 cycle. Le nombre de cycle oscillant autour de 1000 cycles, nous avons donc une erreur de seulement 2%. Notre modèle possède donc de bonnes performances et est donc viable pour le calcul de la durée de vie résiduelle d'une batterie.\n"
      ],
      "metadata": {
        "id": "dJhtVwZxrtP7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "VII-Conclusion\n",
        "\n",
        "\n",
        "Pour conclure, notre modèle présente de bonnes qualités. Il peut néanmoins être amélioré en utilisant d'autres types de modèles que les trois utilisés lors de ce projet. Par exemple, l'utilisation d'un réseau de neuronne qui au vu des thèses faites sur le sujet est plus répendu. De plus nous pourrons également utilisé une autre technioque pour l'optimisation des hyperparamètres. Nous pourrons également améliorer la partie feature engineering en rajoutant des features qui apporteraient plus d'informations à notre dataset."
      ],
      "metadata": {
        "id": "Z8X-meq6ryfP"
      }
    }
  ]
}